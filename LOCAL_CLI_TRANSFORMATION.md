# Codebuff Fork 改造目标与方案

本文档基于前期讨论整理，仅描述改造目标和方案，**不涉及具体代码修改**。适用于在本 fork 分支上开发「本地 CLI 工具」形态的改造。

---

## 一、改造目标

### 1.1 总体目标

在**保持现有两进程架构**（CLI 进程 + 后端进程）的前提下，将本 fork 改造成可完全本地使用的 CLI 工具形态：

- 所有模型调用由**用户自行配置**的 OpenAI 兼容 API 完成，不依赖 Codebuff 官方计费与密钥体系。
- 计费模块不再生效，改为空实现，便于本地/自托管使用。

### 1.2 具体目标

| 序号 | 目标 | 说明 |
|------|------|------|
| 1 | **模型配置** | 所有 Agent 均通过「OpenAI 方式」配置所使用的 LLM：API Key + 可选 Base URL（或等效配置）。 |
| 2 | **Agent 与 LLM 解耦** | Agent 与 LLM 的对应关系由**配置**决定：每个 Agent 可单独指定使用哪套 LLM；多 Agent 可共用同一 LLM，也可各自使用不同 LLM。 |
| 3 | **统一配置文件** | 上述模型与 Agent 的配置集中放在**一个配置文件**中，便于维护与版本管理。 |
| 4 | **计费置空** | 后端侧计费相关逻辑（额度校验、扣费、订阅等）改为空实现或短路，不再依赖计费数据与外部计费服务。 |

### 1.3 明确不做的范围（当前阶段）

- **OpenRouter**：不纳入本阶段改造；若需多模型路由，可由用户在配置中为不同 Agent 指定不同端点实现。
- **Ollama / 本地模型**：后期可能支持；本阶段仅保证「OpenAI 兼容 API」的配置能力，为后续接入 Ollama 等预留扩展空间（同一套配置结构即可）。
- **架构形态**：不改为「无后端直连」；继续采用「CLI 进程 + 后端进程」的现有设计，后端负责运行 Agent 循环并调用 LLM。

---

## 二、架构与角色（保持不变）

- **进程 1 - 后端**：Next.js Web 服务（如 `bun run start-web`）。负责接收 CLI 请求、执行 Agent 运行循环（`loopAgentSteps` 等）、根据配置调用对应 LLM、将流式结果返回 CLI；工具执行结果由 CLI 在本地执行后回传。
- **进程 2 - CLI**：用户直接使用的命令行客户端（如 `bun run start-cli`）。负责与用户交互、将请求与上下文发给后端、接收流式响应、在本地执行工具（读文件、终端命令等）并回传结果。

后端与 CLI 通过 HTTP/WebSocket 通信（如 localhost），现有职责划分保持不变。

---

## 三、改造方案概要

### 3.1 模型与 Agent 配置

- **配置内容**（建议放在单一配置文件，如 YAML/JSON）：
  - **模型端点**：多组「逻辑名称 + base URL + API Key（可选）+ 默认 model id（可选）」；用于兼容 OpenAI、自建 OpenAI 兼容服务及后续 Ollama 等。
  - **Agent 与模型绑定**：为每个 Agent（或 Agent 类型）指定使用哪一组「模型端点」及可选的 model id；未指定时可使用全局默认端点。
- **运行时行为**：后端在需要调用 LLM 时，根据当前 Agent 查询该配置文件，使用对应 base URL 与 API Key 发起 OpenAI 兼容请求；不再使用服务端环境变量中的 Codebuff/OpenRouter 等密钥。

这样实现「所有 Agent 均可通过 OpenAI 方式配置」「Agent 与 LLM 由配置决定、可同可不同」的目标。

### 3.2 计费模块

- **后端**：所有与计费相关的逻辑（额度查询、扣减、订阅校验、套餐限制等）改为空实现或短路，始终视为「有权限、不扣费」；若存在写库或调用外部计费服务，可改为 no-op 或跳过。
- **CLI**：若当前会因「无额度」等计费原因拦截请求，需改为在「本地/自托管模式」下不再依赖计费结果，或不再发起计费相关请求。

不要求实现新的计费逻辑，仅「置空」现有计费路径。

### 3.3 认证与登录（建议）

- 在「本地 CLI 工具」形态下，建议支持**免登录**或**本地模式**：不强制 Codebuff 账号登录即可使用；模型鉴权完全依赖上述配置文件中的 API Key。
- 若保留登录入口，可仅用于可选能力（如同步设置、历史等），不与「能否调用 LLM」强绑定。

具体是否完全取消登录、或保留可选登录，可在实现阶段按产品需求再定。

### 3.4 Agent 来源（建议）

- 默认以**本地 Agent 定义**（如 `.agents/`）及**内置 bundled agents** 为主，不依赖「从 Codebuff 中心/Store 拉取已发布 Agent」。
- 若当前启动或运行会强依赖远程 Agent 拉取，建议在 fork 中改为可选或关闭，避免本地使用时报错或不可用。

---

## 四、配置文件设计（建议）

以下为建议的配置结构，便于后续实现时参考；具体字段与格式可在开发时微调。

- **全局默认**：`default_base_url`、`default_api_key`、`default_model`（可选），用于未单独指定端点的 Agent。
- **模型端点列表**：如 `endpoints[]`，每项含：`name`、`base_url`、`api_key`（可选）、`model`（可选）。
- **Agent 映射**：如 `agents[]` 或 `agent_model_binding`，每项含：Agent id（或 pattern）与使用的 `endpoint` 名称（或直接写 `base_url` + `api_key`）；未列出的 Agent 使用全局默认。

配置文件放置位置、命名（如 `codebuff.local.yaml`）、以及是放在项目目录还是用户目录，可在实现时统一约定。

---

## 五、实施时注意事项

- **不修改代码**：本文档仅作目标与方案说明，具体实现需在后续开发中按本方案落地。
- **兼容性**：改造时尽量保持与现有 Agent 定义、工具、MCP 等兼容；仅替换「模型调用来源」与「计费行为」。
- **文档与注释**：实现时建议在关键处（如读取配置、调用 LLM、计费短路）增加注释或简短说明，标明「本地 CLI fork」下的行为，便于后续维护与合并上游更新。

---

## 六、后续可扩展方向

- **Ollama / 本地模型**：在配置中增加指向本地 Ollama（或其它 OpenAI 兼容服务）的 endpoint 即可，无需调整架构。
- **多环境**：同一配置文件可支持多 profile（如 dev / local / ollama），通过环境变量或 CLI 参数切换。
- **上下文与存储**：当前保持「本地文件存储、无向量库」；若后续需要长对话压缩或检索，可单独规划。

---

*文档版本：基于讨论整理，不修改代码。*
